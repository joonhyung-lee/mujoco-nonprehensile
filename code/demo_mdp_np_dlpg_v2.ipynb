{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a RL Environment about `Non-prehensile task` on table-top scene training with `Deep Latent Policy Gradient`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:[1.13.1+cu116]\n",
      "MuJoCo version:[2.3.4]\n"
     ]
    }
   ],
   "source": [
    "import mujoco,torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mujoco_parser import MuJoCoParserClass\n",
    "from util import r2rpy, sample_xyzs\n",
    "from np_env import NonPrehensileMarkovDecisionProcessClass\n",
    "np.set_printoptions(precision=2,suppress=True,linewidth=100)\n",
    "plt.rc('xtick',labelsize=6); plt.rc('ytick',labelsize=6)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "print (\"Torch version:[%s]\"%(torch.__version__))\n",
    "print (\"MuJoCo version:[%s]\"%(mujoco.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse `UR5e`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UR5e] Instantiated\n",
      "   [info] dt:[0.0200] HZ:[50], env-HZ:[500], mujoco_nstep:[10], state_dim:[26], o_dim:[260], a_dim:[7]\n",
      "   [history] total_sec:[1.00]sec, n:[50], intv_sec:[0.10]sec, intv_tick:[5]\n",
      "   [history] ticks:[ 0  5 10 15 20 25 30 35 40 45]\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "xml_path = '../asset/ur5e/scene_ur5e_rg2_obj.xml'\n",
    "env = MuJoCoParserClass(name='UR5e',rel_xml_path=xml_path,VERBOSE=False)\n",
    "# Instantiate MDP\n",
    "mdp = NonPrehensileMarkovDecisionProcessClass(env,HZ=50,history_total_sec=1.0,history_intv_sec=0.1,VERBOSE=True)\n",
    "\n",
    "obj_names = [body_name for body_name in env.body_names\n",
    "             if body_name is not None and (body_name.startswith(\"obj_\"))]\n",
    "n_obj = len(obj_names)\n",
    "# Place objects\n",
    "xyzs = sample_xyzs(n_sample=n_obj,\n",
    "                   x_range=[0.72,0.95],y_range=[-0.38,0.38],z_range=[0.81,0.81],min_dist=0.2)\n",
    "colors = np.array([plt.cm.gist_rainbow(x) for x in np.linspace(0,1,n_obj)])\n",
    "for obj_idx,obj_name in enumerate(obj_names):\n",
    "    jntadr = env.model.body(obj_name).jntadr[0]\n",
    "    env.model.joint(jntadr).qpos0[:3] = xyzs[obj_idx,:]\n",
    "    geomadr = env.model.body(obj_name).geomadr[0]\n",
    "    env.model.geom(geomadr).rgba = colors[obj_idx] # color\n",
    "\n",
    "# Move tables and robot base\n",
    "env.model.body('base_table').pos = np.array([0,0,0])\n",
    "env.model.body('front_object_table').pos = np.array([1.05,0,0])\n",
    "env.model.body('side_object_table').pos = np.array([0,-0.85,0])\n",
    "env.model.body('base').pos = np.array([0,0,0.8])\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdp.env.ctrl_ranges:\n",
      " [[-2.  2.]\n",
      " [-2.  2.]\n",
      " [-2.  2.]\n",
      " [-2.  2.]\n",
      " [-2.  2.]\n",
      " [-2.  2.]\n",
      " [-2.  2.]]\n"
     ]
    }
   ],
   "source": [
    "max_torque = 2\n",
    "mdp.env.ctrl_ranges[:,0] = -max_torque\n",
    "mdp.env.ctrl_ranges[:,1] = +max_torque\n",
    "print (\"mdp.env.ctrl_ranges:\\n\",mdp.env.ctrl_ranges)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate `DLPG` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random as rd \n",
    "import math \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import wandb\n",
    "import sys \n",
    "# from pathlib import Path\n",
    "# BASEDIR = str(Path(__file__).parent)\n",
    "# sys.path.append(BASEDIR)\n",
    "sys.path.append('..')\n",
    "\n",
    "from model.dlpg.dlpg import DeepLatentPolicyGradient\n",
    "from model.dlpg.buffer import BufferClass\n",
    "from model.utils import torch2np, np2torch, kernel_se, kernel_levse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 122500\n"
     ]
    }
   ],
   "source": [
    "training_data = [json.loads(line) for line in open('./json/np_buffer_v1.json', 'r')]\n",
    "print(\"Total: {}\".format(len(training_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample-Method:uniform\n",
      "Device cuda:0\n",
      "Model Instance.\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample-Method:{}\".format('uniform'))\n",
    "# Set random seed \n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "rd.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device\", device)\n",
    "buffer = BufferClass(xdim=9, cdim=72, buffer_limit=len(training_data), device=device)\n",
    "for idx, data in enumerate(training_data): \n",
    "    buffer.store(x=np.array(data[\"x\"]).reshape(-1), c=data[\"c\"], reward=data[\"reward\"])\n",
    "\n",
    "DLPG = DeepLatentPolicyGradient(xdim     = 9,\n",
    "                                cdim     = 72,\n",
    "                                zdim     = 2,\n",
    "                                hdims    = [128],\n",
    "                                actv_enc = nn.LeakyReLU(),\n",
    "                                actv_dec = None,#nn.LeakyReLU(), \n",
    "                                actv_out = nn.Tanh(), \n",
    "                                actv_q   = nn.Softplus(),\n",
    "                                device   = device)\n",
    "# DLPG.cvae.load_state_dict(torch.load(weight_path))\n",
    "optimizer = torch.optim.Adam(params=DLPG.cvae.parameters(),lr=0.001,betas=(0.9,0.99),eps=1e-4)\n",
    "\n",
    "print(\"Model Instance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "batch_size = 128\n",
    "epsgrdy = 0.1\n",
    "\n",
    "# wandb.init(project=\"dlpg\", entity=\"dlpg\")\n",
    "# wandb.config.max_epochs = max_epochs\n",
    "# wandb.config.batch_size = batch_size\n",
    "\n",
    "eval_epochs = 10\n",
    "n_sample = 10\n",
    "RENDER = True\n",
    "WANDB = False\n",
    "update_every = 1\n",
    "MAXITER = 1000\n",
    "sample_method = 'uniform'\n",
    "runname = 'none'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_pose(env, obj_names, x_range=[0.72,0.95],y_range=[-0.38,0.38],z_range=[0.81,0.81],min_dist=0.2):\n",
    "    \"\"\"\n",
    "        Obstacle random spawn \n",
    "    \"\"\"\n",
    "    n_obj = len(obj_names)\n",
    "    # Place objects\n",
    "    xyzs = sample_xyzs(n_sample=n_obj,\n",
    "                    x_range=x_range,y_range=y_range,z_range=z_range,min_dist=min_dist)\n",
    "    colors = np.array([plt.cm.gist_rainbow(x) for x in np.linspace(0,1,n_obj)])\n",
    "    for obj_idx,obj_name in enumerate(obj_names):\n",
    "        jntadr = env.model.body(obj_name).jntadr[0]\n",
    "        env.model.joint(jntadr).qpos0[:3] = xyzs[obj_idx,:]\n",
    "        geomadr = env.model.body(obj_name).geomadr[0]\n",
    "        env.model.geom(geomadr).rgba = colors[obj_idx] # color\n",
    "\n",
    "    return xyzs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"[Epoch: {}]\".format(epoch+1))\n",
    "    \"\"\" ROLLOUT \"\"\"\n",
    "    env.reset()\n",
    "    # Ranodm spawn\n",
    "    goal_position  = np.random.uniform(-0.1, 0.1) \n",
    "    # random_obs = env.random_pose(goal_position=goal_position)\n",
    "    random_obs = random_pose(env, obj_names, x_range=[0.72,0.95],y_range=[-0.38,0.38],z_range=[0.81,0.81],min_dist=0.2)\n",
    "    \n",
    "    # Get a conditional vector\n",
    "    c_np           = np.array(random_obs).reshape(-1)\n",
    "    c_np           = np.append(c_np, goal_position) # [(x,y) x n + 1 (y axis of goal posiiton)]\n",
    "    c              = np2torch(c_np, device=device).reshape(1,-1)\n",
    "    # Epsgrdy\n",
    "    EXPLORE        = 1/10**(epoch/epsgrdy)   \n",
    "    EXPLOIT        = np.random.rand() > EXPLORE \n",
    "    # Exploit [Posterior sampling]\n",
    "    if EXPLOIT:\n",
    "        z                              = torch.randn(size=(1, DLPG.zdim)).to(device)\n",
    "        traj, t_test, normed_x_anchor  = DLPG.exploit(z=z, c=c, goal_position=goal_position)\n",
    "        normed_x_anchor                = normed_x_anchor.reshape(-1)\n",
    "        x_anchor                       = DLPG.scale_up(normed_x_anchor) # Store it into the buffer \n",
    "        # Solve IK \n",
    "        q_trajs  = []\n",
    "        for _, (x, y) in enumerate(zip(t_test, traj)):\n",
    "            # Make a trajectory by adding q values in every step.\n",
    "            q = env.solve_ik(P_EE_des=np.array([x, y, 0.9], dtype=object), \n",
    "                                R_EE_des=np.array([-math.pi, 0, math.pi], dtype=object))\n",
    "            q_trajs.append(q)        \n",
    "            # Target position\n",
    "            goal = np.array([x, y, 0.9], dtype=object)\n",
    "            # Interpolation \n",
    "            interpoled_q_traj = DLPG.grp.interpolation(x_anchor=q_trajs, num_interpol=5)    \n",
    "            # Render \n",
    "            collision = env.execute_arm(q_des_lst    = interpoled_q_traj, \n",
    "                                        gripper_mode = \"open\", \n",
    "                                        goal         = goal, \n",
    "                                        obs_pose_lst = random_obs, \n",
    "                                        RENDER       = RENDER)\n",
    "            reward = DLPG.get_reward(collision, normed_x_anchor, random_obs, goal_position) \n",
    "            buffer.store(x=x_anchor.reshape(-1), c=c, reward=reward)        \n",
    "    # Explore [Prior sampling]\n",
    "    else: \n",
    "        # trajs, t_test = DLPG.random_explore(n_sample=n_sample, goal_position=goal_position)\n",
    "        trajs, t_test = DLPG.random_explore(n_sample=n_sample)\n",
    "        # Solve IK \n",
    "        for traj in trajs:\n",
    "            q_trajs  = []\n",
    "            normed_x_anchor = traj[5:7] # Fix index, To sample a point around a position of the target object.\n",
    "            x_anchor        = DLPG.scale_up(normed_x_anchor) # Store it into the buffer \n",
    "            for _, (x, y) in enumerate(zip(t_test, traj)):\n",
    "                # Make a trajectory by adding q values in every step.\n",
    "                q = env.solve_ik(P_EE_des=np.array([x, y, 0.9], dtype=object), \n",
    "                                    R_EE_des=np.array([-math.pi, 0, math.pi], dtype=object))\n",
    "                q_trajs.append(q)\n",
    "            # Target position\n",
    "            goal = np.array([x, y, 0.9], dtype=object)\n",
    "            # Interpolation \n",
    "            interpoled_q_traj = DLPG.grp.interpolation(x_anchor=q_trajs, num_interpol=5)    \n",
    "            # Render \n",
    "            collision = env.execute_arm(q_des_lst    = interpoled_q_traj, \n",
    "                                        gripper_mode = \"open\", \n",
    "                                        goal         = goal, \n",
    "                                        obs_pose_lst = random_obs, \n",
    "                                        RENDER       = RENDER)\n",
    "            reward = DLPG.get_reward(collision, normed_x_anchor, random_obs, goal_position) \n",
    "            buffer.store(x=x_anchor.reshape(-1), c=c_np, reward=reward)\n",
    "    \"\"\" UPDATE \"\"\"\n",
    "    loss_recon_sum=0;loss_kl_sum=0;n_batch_sum=0\n",
    "    if (epoch+1)%update_every==0 and (epoch+1)>99:\n",
    "        for it in range(MAXITER):\n",
    "            if it >= 30: beta = 10 # Heuristic \n",
    "            else:        beta = 0.0\n",
    "            batch = buffer.sample_batch(sample_method=sample_method, batch_size=batch_size)\n",
    "            x_batch, c_batch, reward_batch = batch[\"x\"], batch[\"c\"], batch[\"reward\"]\n",
    "            total_loss_out,loss_info = DLPG.cvae.loss_total(x               = x_batch, \n",
    "                                                            c               = c_batch, \n",
    "                                                            q               = reward_batch, \n",
    "                                                            LOSS_TYPE       = 'L1+L2',\n",
    "                                                            recon_loss_gain = 1,\n",
    "                                                            beta            = beta,\n",
    "                                                            STOCHASTICITY   = True)\n",
    "            optimizer.zero_grad()\n",
    "            total_loss_out.backward()\n",
    "            optimizer.step()\n",
    "            n_batch        = x_batch.shape[0]\n",
    "            loss_recon_sum = loss_recon_sum + n_batch*loss_info['loss_recon_out']\n",
    "            loss_kl_sum    = loss_kl_sum + n_batch*loss_info['loss_kl_out']\n",
    "            n_batch_sum    = n_batch_sum + n_batch\n",
    "        # Average loss during train\n",
    "        loss_recon_avg, loss_kl_avg = (loss_recon_sum/n_batch_sum),(loss_kl_sum/n_batch_sum)\n",
    "        # Print\n",
    "        print (\"[%d/%d] DLPG updated. Total loss:[%.3f] (recon:[%.3f] kl:[%.3f])\"%\n",
    "            (epoch+1,max_epochs,loss_recon_avg+loss_kl_avg,loss_recon_avg,loss_kl_avg))\n",
    "        if WANDB:\n",
    "            wandb.log({\"Total loss\":loss_recon_avg+loss_kl_avg,\n",
    "                        \"recon_loss\":loss_recon_avg,\n",
    "                        \"kl_loss\":loss_kl_avg}, step=epoch+1)   \n",
    "        # Save weights         \n",
    "        torch.save(DLPG.cvae.state_dict(),\"weights\"+\"/\"+str(runname)+\"/{}steps.pth\".format(epoch+1))    \n",
    "        \"\"\" EVALUATE \"\"\"\n",
    "        eval_reward=0\n",
    "        plt.figure(figsize=(6,9))\n",
    "        with torch.no_grad():\n",
    "            for it in range(eval_epochs):\n",
    "                env.reset()\n",
    "                # Random spawn \n",
    "                goal_position  = np.random.uniform(-0.1, 0.1) \n",
    "                random_obs = env.random_pose(goal_position=goal_position)\n",
    "                # Get a conditional vector\n",
    "                c_np           = np.array(random_obs).reshape(-1)\n",
    "                c_np           = np.append(c_np, goal_position)  \n",
    "                c              = np2torch(c_np, device=device).reshape(1,-1)\n",
    "                z              = torch.randn(size=(1, DLPG.zdim)).to(device)\n",
    "                traj, t_test, normed_x_anchor = DLPG.exploit(z=z, \n",
    "                                                                c=c, \n",
    "                                                                goal_position=goal_position)   \n",
    "                normed_x_anchor = normed_x_anchor.reshape(-1)\n",
    "                # Solve IK \n",
    "                q_trajs  = []\n",
    "                for _, (x, y) in enumerate(zip(t_test, traj)):\n",
    "                    # Make a trajectory by adding q values in every step.\n",
    "                    q = env.solve_ik(P_EE_des=np.array([x, y, 0.9], dtype=object), \n",
    "                                        R_EE_des=np.array([-math.pi, 0, math.pi], dtype=object))\n",
    "                    q_trajs.append(q)\n",
    "                # Target position\n",
    "                goal = np.array([x, y, 0.9], dtype=object)\n",
    "                # Interpolation \n",
    "                interpoled_q_traj = DLPG.grp.interpolation(x_anchor=q_trajs, num_interpol=10)    \n",
    "                # Render \n",
    "                collision = env.execute_arm(q_des_lst    = interpoled_q_traj, \n",
    "                                            gripper_mode = \"open\", \n",
    "                                            goal         = goal, \n",
    "                                            obs_pose_lst = random_obs, \n",
    "                                            RENDER       = RENDER)\n",
    "                reward    = DLPG.get_reward(collision, normed_x_anchor, random_obs, goal_position)  \n",
    "                eval_reward+=reward\n",
    "                # Demonstration for exploitation samples\n",
    "                plt.ylim(-0.45, 0.45)\n",
    "                plt.xlim(0.5,0.9)\n",
    "                plt.title(\"Exploit samples[Epoch{}]\".format(epoch), fontsize=20)\n",
    "                plt.xlabel(\"X axis\", fontsize=15)\n",
    "                plt.ylabel(\"Y axis\", fontsize=15)\n",
    "                plt.scatter(0.65, normed_x_anchor[0], s=50) \n",
    "                plt.scatter(0.8,  normed_x_anchor[1], s=50)\n",
    "            plt.savefig(\"data/exploit_samples{}.png\".format(epoch+1))\n",
    "            # Print\n",
    "            if WANDB: wandb.log({\"Reward\":eval_reward/eval_epochs}, step=epoch+1)   \n",
    "            print(\"[Evaluate Reward]:{}\".format(eval_reward/eval_epochs))\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
